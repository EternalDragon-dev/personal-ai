# Personal AI Framework

> Build your own personalized AI assistant with custom models, RAG, and conversation memory

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)
[![Status](https://img.shields.io/badge/Status-Active%20Development-yellow)](https://github.com)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

[**ğŸ“¸ Screenshot Coming Soon**]

## Overview

A modular framework for building personalized AI assistants from scratch. Supports custom model training, retrieval-augmented generation (RAG), conversation memory, and extensible plugin architecture.

**Perfect for:**
- Learning how AI assistants work under the hood
- Building custom AI solutions for personal use
- Experimenting with different AI models and techniques
- Privacy-focused AI with local processing

## Features

- ğŸ¤– **Custom AI Engine** - Integrate any model (HuggingFace, OpenAI, Ollama)
- ğŸ’¾ **Conversation Memory** - Persistent context across sessions
- ğŸ“š **RAG Support** - Retrieval-augmented generation for knowledge bases
- ğŸ”Œ **Plugin Architecture** - Extensible with custom tools and skills
- ğŸ¯ **CLI Interface** - Simple command-line interaction
- ğŸ›¡ï¸ **Privacy-First** - Local processing option, no data leaks
- âš™ï¸ **Modular Design** - Easy to customize and extend

## Quick Start

### Prerequisites

- Python 3.8 or higher
- pip package manager
- (Optional) Ollama for local AI models

### Installation

1. **Clone and navigate:**
   ```bash
   cd personal-ai
   ```

2. **Create virtual environment:**
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the CLI:**
   ```bash
   python src/cli.py
   ```

### First Conversation

```bash
$ python src/cli.py

ğŸ¤– Personal AI - Ready to chat!
Type 'exit' to quit, 'clear' to reset conversation

You: Hello! What can you do?
AI: I'm your personal AI assistant. I can help with...

You: Tell me about Python
AI: Python is a high-level programming language...
```

## Architecture

```
personal-ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ ai_engine.py      # Core AI model integration
â”‚   â”‚   â”œâ”€â”€ memory.py          # Conversation memory manager
â”‚   â”‚   â””â”€â”€ embeddings.py      # Text embeddings for RAG
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ base.py            # Base model interface
â”‚   â”‚   â”œâ”€â”€ ollama.py          # Ollama integration
â”‚   â”‚   â””â”€â”€ openai.py          # OpenAI integration
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ processor.py       # Data preprocessing
â”‚   â”‚   â””â”€â”€ vectorstore.py     # Vector database
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”‚   â””â”€â”€ logger.py          # Logging utilities
â”‚   â”œâ”€â”€ cli.py                 # Command-line interface
â”‚   â””â”€â”€ main.py                # Main application
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml           # Default configuration
â”œâ”€â”€ data/                      # User data & documents
â”œâ”€â”€ models/                    # Downloaded model files
â””â”€â”€ logs/                      # Application logs
```

## Usage Examples

### Basic Chat

```python
from src.core.ai_engine import AIEngine

engine = AIEngine(model="ollama/llama2")
response = engine.chat("What is machine learning?")
print(response)
```

### Conversation with Memory

```python
from src.core.ai_engine import AIEngine
from src.core.memory import ConversationMemory

memory = ConversationMemory()
engine = AIEngine(memory=memory)

engine.chat("My name is John")
engine.chat("What's my name?")  # AI remembers context
```

### RAG with Documents

```python
from src.core.ai_engine import AIEngine
from src.data.vectorstore import VectorStore

vectorstore = VectorStore()
vectorstore.add_documents(["path/to/docs"])

engine = AIEngine(vectorstore=vectorstore)
response = engine.chat("Summarize the documentation")
```

## Configuration

### AI Model Settings

Edit `config/default.yaml`:

```yaml
ai:
  provider: ollama  # Options: ollama, openai, huggingface
  model: llama2
  temperature: 0.7
  max_tokens: 500

memory:
  enabled: true
  max_history: 10
  
rag:
  enabled: false
  chunk_size: 500
  overlap: 50
```

### Environment Variables

```bash
# For OpenAI (if using)
export OPENAI_API_KEY=your_key_here

# For HuggingFace (if using)
export HUGGINGFACE_TOKEN=your_token_here
```

## Tech Stack

- **Language:** Python 3.8+
- **AI Integration:** HuggingFace Transformers, OpenAI API, Ollama
- **Embeddings:** Sentence Transformers
- **Vector Store:** ChromaDB / FAISS
- **CLI:** Click / argparse
- **Testing:** pytest

## Project Status

- [x] Project structure and architecture
- [x] Configuration system
- [x] Logging utilities
- [ ] **Core AI engine** (IN PROGRESS)
- [ ] Conversation memory
- [ ] RAG implementation
- [ ] CLI interface
- [ ] Plugin system
- [ ] Web interface
- [ ] Documentation

## Roadmap

### Phase 1: Core Functionality (Current)
- Implement AI engine with model loading
- Add basic conversation memory
- Create functional CLI

### Phase 2: Advanced Features
- RAG with document ingestion
- Plugin architecture
- Multiple model support

### Phase 3: Polish
- Web interface
- Model fine-tuning tools
- Comprehensive documentation

## Development

### Run in development mode:
```bash
source .venv/bin/activate
python src/cli.py --debug
```

### Run tests:
```bash
pytest tests/ -v
```

### Code formatting:
```bash
black src/
flake8 src/
```

## Troubleshooting

**Import errors:**
```bash
source .venv/bin/activate
pip install -r requirements.txt
```

**Model loading slow:**
- First load downloads models (can take time)
- Models are cached for subsequent runs
- Use smaller models for faster startup

**Memory issues:**
- Reduce max_tokens in config
- Use quantized models
- Clear conversation history regularly

## Contributing

Contributions welcome! This project is actively being developed.

**Areas needing work:**
- Core AI engine implementation
- RAG system
- Additional model integrations
- Documentation
- Tests

## License

MIT License - Free to use and modify!

## Resources

- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [Ollama Documentation](https://ollama.ai/docs)
- [LangChain](https://python.langchain.com/) - Inspiration for architecture
- [ChromaDB](https://www.trychroma.com/) - Vector database
